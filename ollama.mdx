---
title: Ollama Integration
description: Configure and use Ollama for running local LLM models with Laddr agents.
---

# Ollama Integration

Run Laddr agents with local LLM models using Ollama. This guide covers configuration, Docker setup, and usage examples.

## Configuration

### Environment Variables

Add these to your `.env` file:

<CodeBlock code={`# ===================================
# Ollama Configuration
# ===================================

# Ollama server URL
# For local development (Ollama running on host):
OLLAMA_BASE_URL=http://localhost:11434

# For Docker containers (see Docker Setup section):
# OLLAMA_BASE_URL=http://host.docker.internal:11434

# Default Ollama model (used if not specified per-agent)
OLLAMA_MODEL=gemma2:2b

# ===================================
# Backend Selection
# ===================================

# Set Ollama as default LLM backend for all agents
LLM_BACKEND=ollama

# Or configure per-agent:
LLM_BACKEND_RESEARCHER=ollama
LLM_BACKEND_COORDINATOR=openai  # Mix with cloud models

# ===================================
# Per-Agent Models
# ===================================

# Use different Ollama models for different agents
LLM_MODEL_RESEARCHER=gemma2:2b        # Fast, cheap model
LLM_MODEL_COORDINATOR=llama3.2:latest # Better reasoning
LLM_MODEL_WRITER=mistral:7b           # Good at writing`} language="bash" />

### Configuration Priority

Laddr resolves Ollama configuration in this order (highest to lowest priority):

**For base URL:**
1. Per-agent base URL: `LLM_BASE_URL_RESEARCHER=http://custom:11434`
2. Global Ollama URL: `OLLAMA_BASE_URL=http://localhost:11434`
3. Default: `http://localhost:11434`

**For models:**
1. Per-agent model: `LLM_MODEL_RESEARCHER=gemma2:2b`
2. Global Ollama model: `OLLAMA_MODEL=llama3.2:latest`
3. Global LLM model: `LLM_MODEL=gemma2:2b`
4. Default: `gemma2:2b`

---

## Docker Setup

When running Laddr in Docker containers, special network configuration is needed so containers can reach Ollama running on the host machine.

### Problem: Container Network Isolation

By default, `http://localhost:11434` inside a Docker container refers to the **container itself**, not the host machine where Ollama is running.

**Error you might see:**

<CodeBlock code={`LLM generation failed: Ollama endpoints not reachable at http://localhost:11434
Cannot connect to host localhost:11434`} language="text" />

### Solution: Use host.docker.internal

Docker provides a special hostname `host.docker.internal` that resolves to the host's IP address.

### Docker Compose Configuration

Update your `docker-compose.yml`:

<CodeBlock code={`services:
  api:
    build:
      context: ..
      dockerfile: tester/Dockerfile
    ports:
      - 8000:8000
    environment:
      # Point to host's Ollama via special hostname
      OLLAMA_BASE_URL: "http://host.docker.internal:11434"
    # Map host.docker.internal to host gateway
    extra_hosts:
      - "host.docker.internal:host-gateway"
    # ... other config

  researcher_worker:
    build:
      context: ..
      dockerfile: tester/Dockerfile
    command: python -m agents.researcher
    environment:
      AGENT_NAME: researcher
      # Worker must also use host.docker.internal
      OLLAMA_BASE_URL: "http://host.docker.internal:11434"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    # ... other config

  coordinator_worker:
    build:
      context: ..
      dockerfile: tester/Dockerfile
    command: python -m agents.coordinator
    environment:
      AGENT_NAME: coordinator
      OLLAMA_BASE_URL: "http://host.docker.internal:11434"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    # ... other config`} language="yaml" />

### Key Configuration Points

**1. Environment Variable**

<CodeBlock code={`environment:
  OLLAMA_BASE_URL: "http://host.docker.internal:11434"`} language="yaml" />

Tells Laddr to connect to Ollama via the special hostname.

**2. Extra Hosts Mapping**

<CodeBlock code={`extra_hosts:
  - "host.docker.internal:host-gateway"`} language="yaml" />

Maps the hostname to the Docker host's gateway IP.

**3. All Services Need It**

Apply this configuration to **all services** that use Ollama:
- API server
- Worker containers
- Any agent services

### Verification in Docker

Test Ollama connectivity from inside a container:

<CodeBlock code={`# Start your containers
docker compose up -d

# Exec into a worker container
docker compose exec researcher_worker bash

# Test connection
curl http://host.docker.internal:11434/api/generate -d '{
  "model": "gemma2:2b",
  "prompt": "test",
  "stream": false
}'`} language="bash" />

Expected: JSON response with generated text.

---

## Usage Examples

### Example 1: Simple Agent with Ollama

<CodeBlock code={`# my_agent.py
from laddr import Agent
from laddr.llms import ollama

researcher = Agent(
    name="researcher",
    role="Research Specialist",
    goal="Find and analyze information",
    backstory="You are an expert researcher.",
    
    # Use Ollama with gemma2:2b model
    llm=ollama(model="gemma2:2b"),
    
    tools=[],
    instructions="Research the given topic thoroughly."
)`} language="python" />

### Example 2: Custom Ollama Server

<CodeBlock code={`from laddr import Agent
from laddr.llms import ollama

# Connect to Ollama on custom port/host
researcher = Agent(
    name="researcher",
    role="Researcher",
    goal="Research information",
    backstory="Expert researcher",
    
    llm=ollama(
        model="llama3.2:latest",
        base_url="http://192.168.1.100:11434"  # Remote Ollama server
    ),
    
    tools=[]
)`} language="python" />

### Example 3: Mixed Backends

<CodeBlock code={`from laddr import Agent
from laddr.llms import ollama, gemini

# Coordinator uses cloud API (complex reasoning)
coordinator = Agent(
    name="coordinator",
    role="Task Coordinator",
    goal="Coordinate research tasks",
    backstory="Experienced coordinator",
    llm=gemini("gemini-2.0-flash"),  # Cloud API
    is_coordinator=True
)

# Researcher uses local Ollama (cost-effective)
researcher = Agent(
    name="researcher",
    role="Researcher",
    goal="Find information",
    backstory="Research specialist",
    llm=ollama("gemma2:2b"),  # Local model
    tools=[web_search]
)`} language="python" />

### Example 4: Environment-Based Configuration

<CodeBlock code={`# agents/researcher.py
from laddr import Agent
from laddr.llms import ollama

# Model and URL from environment variables
researcher = Agent(
    name="researcher",
    role="Researcher",
    goal="Research topics",
    backstory="Expert researcher",
    
    # Automatically uses:
    # - LLM_MODEL_RESEARCHER or OLLAMA_MODEL or default
    # - OLLAMA_BASE_URL or default
    llm=ollama(),
    
    tools=[]
)`} language="python" />

---
